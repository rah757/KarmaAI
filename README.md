# Karma AI ‚Äì Multimodal Prompting for the Visually Impaired using Vertex AI

Karma AI is an assistive multimodal AI system developed to provide real-time contextual assistance to visually impaired individuals. It utilizes state-of-the-art technologies, integrating voice commands, advanced image analysis, and natural language processing powered by Google Cloud's Vertex AI and OpenAI, to deliver accessible audio feedback about the user's surroundings.

## Key Functionalities

- **Voice Interaction**: Users communicate naturally via speech, initiating tasks and receiving clear audio responses.
- **Real-time Image Recognition**: Leverages Google Cloud Vision API and ML Kit for rapid object detection and scene understanding.
- **Emergency Fall Detection**: Built-in accelerometer-based detection system identifies falls, automatically triggering emergency calls and messages if necessary.
- **AI Scene Description**: Captures and interprets images in real-time using Google's Gemini API, providing succinct verbal descriptions tailored for navigation and safety.
- **ChatGPT Assistant**: Integrated GPT-4o-mini model offers concise, helpful guidance through conversational interactions, specifically optimized for visually impaired users.

## Tech Stack

- **Google Cloud Vertex AI**, **Gemini Pro Vision API**
- **OpenAI GPT-4o-mini**
- **Android (CameraX, TTS, Speech Recognition)**
- **Kotlin**, **ML Kit**, **Firebase**

## Award & Recognition

üèÜ **Social Impact Award ‚Äì 2025 Applied AI Challenge**  
Hosted by the **Student Innovation Center at Iowa State University**  
Awarded to Karma AI for effectively addressing accessibility challenges using advanced multimodal AI technologies.

### Team Members:
- Rahman Abdul Rafi
- Ankit Jyothish
- Kunal Suresh
- Arun Kumar Iyyappan
- Mohammed Musthafa Rafi


